{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_error(clf, X, y, k=5):\n",
    "    \"\"\"\n",
    "        Splits the data, X and y, into k-folds. Trains a classifier on K-1 folds and\n",
    "        testing on the remaining fold. Calculates the cross validation train and test\n",
    "        error for classifier clf by averaging the performance across folds. \n",
    "        Input:\n",
    "          clf- an instance of SVC()\n",
    "          X- (n,d) array of feature vectors, where n is # of examples\n",
    "             and d is # of features\n",
    "          y- (n,) array of binary labels {1,-1}\n",
    "          k- int, # of folds (default=5)\n",
    "\n",
    "        Returns: average train and test error across the k folds as np.float64\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(y, k)\n",
    "    train_scores, test_scores = [], []\n",
    "    for train, test in skf:\n",
    "        X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_scores.append(clf.score(X_train, y_train))\n",
    "        test_scores.append(clf.score(X_test, y_test))\n",
    "        \n",
    "    return 1 - np.array(train_scores).mean(), 1 - np.array(test_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_error(X, y): \n",
    "    '''\n",
    "    Plots the variation of 5-fold cross-validation error w.r.t. maximum\n",
    "    depth of the decision tree\n",
    "        X- (n, d) array of feature vectors, where n is # of examples\n",
    "             and d is # of features\n",
    "        y- (n, ) array of labels corresponding to X\n",
    "    '''\n",
    "    \n",
    "    # ---------- make your implementation here---------------\n",
    "    # -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def majority_vote(pred_list):\n",
    "    '''\n",
    "    Given a list of m (n, ) arrays, each (n, ) array containing the predictions for \n",
    "    same set of n examples (using different classifiers), return a (n, ) array \n",
    "    containing majority vote prediction of the m (n, ) arrays\n",
    "    Input:\n",
    "        pred_list- a list of m (n, ) arays\n",
    "    Output:\n",
    "        y_pred- (n, ) array containing majority vote prediction using pred_list\n",
    "    '''\n",
    "    y_pred = []\n",
    "    for i in range(len(pred_list[0])):\n",
    "        lst = [row[i] for row in pred_list]\n",
    "        y_pred.append(max(set(lst), key = lst.count))\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagging_ensemble(X_train, y_train, X_test, y_test, m = None, n_clf = 10):\n",
    "    '''\n",
    "    Returns accuracy on the test set X_test with corresponding labels y_test\n",
    "    using a bagging ensemble classifier with n_clf decision trees trained with \n",
    "    training examples X_train and training labels y_train.\n",
    "    Input:\n",
    "        X_train- (n_train, d) array of training feature vectors, where n_train \n",
    "            is # of examples and d is # of features\n",
    "        y_train- (n_train, ) array of labels corresponding to X_train\n",
    "        X_test- (n_test, d) array of testing feature vectors, where n_test is \n",
    "            # of examples and d is # of features\n",
    "        y_test- (n_test, ) array of labels corresponding to X_test\n",
    "        m - int, # of features to consider when looking for the best split\n",
    "        n_clf- # of decision tree classifiers in the bagging ensemble, default\n",
    "            value of n_clf is 10\n",
    "    Output:\n",
    "        Accuracy of the bagging ensemble classifier on X_test\n",
    "    '''\n",
    "    \n",
    "    # ---------- make your implementation here---------------\n",
    "    return accuracy\n",
    "    # -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test, m, n_clf = 10):\n",
    "    '''\n",
    "    Returns accuracy on the test set X_test with corresponding labels y_test\n",
    "    using a random forest classifier with n_clf decision trees trained with \n",
    "    training examples X_train and training labels y_train.\n",
    "    Input:\n",
    "        X_train- (n_train, d) array of training feature vectors, where n_train \n",
    "            is # of examples and d is # of features\n",
    "        y_train- (n_train, ) array of labels corresponding to X_train\n",
    "        X_test- (n_test, d) array of testing feature vectors, where n_test is \n",
    "            # of examples and d is # of features\n",
    "        y_test- (n_test, ) array of labels corresponding to X_test\n",
    "        n_clf- # decision tree classifiers in the random forest, default\n",
    "            value of n_clf is 10\n",
    "    Output:\n",
    "        Accuracy of the random forest classifier on X_test\n",
    "    '''\n",
    "\n",
    "    # ---------- make your implementation here---------------\n",
    "    return accuracy\n",
    "    # -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_histograms(random_forest_scores, bagging_scores):\n",
    "    '''\n",
    "    Plots histogram of values in random_forest_scores and bagging_scores\n",
    "    overlayed on top of each other\n",
    "    Input:\n",
    "        random_forest_scores- a list containing accuracy values for random forest classifier \n",
    "        for 100 different train and test set splits\n",
    "        bagging_scores- a list containing accuracy values for bagging ensemble classifier \n",
    "        using decision trees for the same 100 different train and test set splits\n",
    "        as random_forest_scores\n",
    "    '''\n",
    "    bins = np.linspace(0.8, 1.0, 100)\n",
    "    plt.figure()\n",
    "    plt.hist(random_forest_scores, bins, alpha=0.5, label='random forest')\n",
    "    plt.hist(bagging_scores, bins, alpha=0.5, label='bagging')\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q5a():\n",
    "    # Load Iris dataset\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data[:,:2], iris.target\n",
    "    # PLot cross-validation error vs max_depth \n",
    "    plot_error(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q5b():\n",
    "    # Load digits dataset\n",
    "    digits = load_digits(4)\n",
    "    X, y = digits.data, digits.target\n",
    "    \n",
    "    # Calculate accuracy of bagging ensemble and random forest for 100 random train/test splits\n",
    "    # Analyze how the performance of bagging & random forest changes with m\n",
    "    results1, results2 = [], []\n",
    "    for j in range(0, 64, 2):\n",
    "        print j\n",
    "        bagging_scores, random_forest_scores = [], []\n",
    "        for i in range(100):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "            random_forest_scores.append(random_forest(X_train, y_train, X_test, y_test, j+1))\n",
    "            bagging_scores.append(bagging_ensemble(X_train, y_train, X_test, y_test))\n",
    "        results1.append(np.median(np.array(random_forest_scores)))\n",
    "        results2.append(np.median(np.array(bagging_scores)))\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.plot(range(0, 64, 2), results1, '--', label = 'random forest')\n",
    "    plt.plot(range(0, 64, 2), results2, '--', label = 'bagging')\n",
    "    plt.xlabel('m')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    #Plot histogram of performances for m = 8\n",
    "    bagging_scores, random_forest_scores = [], []\n",
    "    for i in range(100):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        random_forest_scores.append(random_forest(X_train, y_train, X_test, y_test,8))\n",
    "        bagging_scores.append(bagging_ensemble(X_train, y_train, X_test, y_test))\n",
    "    plot_histograms(random_forest_scores, bagging_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q5a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q5b()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
